## Topic: Configure Workload Autoscaling

### 1. Theory
**Workload autoscaling** in Kubernetes enables applications to adapt to changing demand by adjusting the number of pods or nodes. This topic focuses on the **Horizontal Pod Autoscaler (HPA)**, which scales pods based on metrics like CPU or memory, and briefly covers the **Cluster Autoscaler** for node scaling, providing context for how they interact.

- **Horizontal Pod Autoscaler (HPA)**:
  - Automatically scales the number of pods in a Deployment, ReplicaSet, or StatefulSet.
  - Uses metrics (e.g., CPU, memory, custom) to determine when to scale up or down.
  - Requires `metrics-server` for standard metrics or a custom adapter for others.

- **Cluster Autoscaler**:
  - Scales the number of nodes in the cluster to support pod scheduling.
  - Works with HPA to ensure sufficient capacity for scaled pods.
  - Not a primary CKA focus but relevant for understanding resource constraints.

**Why It Matters for CKA**:
- Autoscaling is a key CKA topic, testing your ability to configure HPAs, simulate load, and troubleshoot scaling issues under time constraints.
- Tasks often involve creating HPAs, validating scaling behavior, or fixing misconfigurations, reflecting real-world needs for elastic applications.

**Big Picture**:
- HPA ensures applications handle load efficiently without manual intervention.
- Cluster Autoscaler supports HPA by providing node capacity, though itâ€™s less exam-critical.
- Troubleshooting scaling failures often involves checking metrics, resources, or pod states, sometimes requiring runtime debugging.

---

### 2. Key Concepts and Components
#### Horizontal Pod Autoscaler (HPA)
- **Purpose**: Scale pods dynamically based on observed metrics.
- **Key Fields** (v2 API, `autoscaling/v2`):
  - **minReplicas**: Minimum number of pods (e.g., `2`).
  - **maxReplicas**: Maximum number of pods (e.g., `10`).
  - **metrics**: Scaling criteria:
    - **resource** (e.g., CPU, memory): Target utilization (e.g., `targetAverageUtilization: 80`).
    - **custom**: External metrics (e.g., requests per second, not CKA-required).
  - **behavior**: Scaling policies (optional):
    - `scaleUp`: Rules for adding pods (e.g., stabilization window).
    - `scaleDown`: Rules for removing pods (e.g., delay to avoid flapping).
- **Prerequisites**:
  - **metrics-server**: Provides CPU/memory metrics (must be installed).
  - **resource requests**: Pods need `requests.cpu` or `requests.memory` defined.
  - Custom metrics: Requires adapter (e.g., Prometheus, not CKA-focused).
- **Scaling Behavior**:
  - HPA checks metrics every 15 seconds (default).
  - Scales pods to meet target (e.g., CPU at 80%).
  - Stabilization windows prevent rapid fluctuations:
    - Scale-up: 3 minutes (default).
    - Scale-down: 5 minutes (default).
- **Example**: Scale a web app from 2 to 10 pods when CPU exceeds 80%.

#### Cluster Autoscaler
- **Purpose**: Add/remove nodes based on pod scheduling needs.
- **Interaction with HPA**:
  - HPA increases pod count; Cluster Autoscaler adds nodes if capacity is insufficient.
  - Removes nodes when pods are scaled down and nodes are underutilized.
- **Key Concepts**:
  - **Node taints/tolerations**: Pods must tolerate node taints to schedule.
  - **Unschedulable pods**: Trigger node scale-up if no nodes fit.
- **CKA Relevance**: Limited, but understanding ensures holistic scaling knowledge.
- **Example**: Add nodes when HPA scales pods beyond current capacity.

#### Dependencies
- **metrics-server**:
  - Lightweight service exposing `/metrics` for pods.
  - Deployed in `kube-system` (e.g., via `kubectl apply -f components.yaml`).
  - Required for CPU/memory-based HPA.
- **Resource Requests**:
  - Pods must define `requests.cpu`/`requests.memory` in `spec.containers.resources`.
  - HPA uses these to calculate utilization (e.g., `used CPU / requested CPU`).

#### Runtime Debugging
- **Use Case**: Troubleshoot HPA failures (e.g., pods not scaling, metrics errors).
- **Tools**:
  - `crictl ps`: Check pod states (e.g., `Running`, `CrashLoopBackOff`).
  - `crictl logs <container-id>`: Fetch app errors.
  - `crictl inspect <container-id>`: Verify resource limits.
  - `journalctl -u kubelet`: Kubelet metrics issues.
- **Relevance**: Diagnose pod crashes or scheduling issues during scaling.

#### Exam Relevance
- **Moderate Weight**: HPA is common, testing scaling setup and debugging.
- **Practical Focus**: Expect to create HPAs, simulate load, fix metrics issues, and validate scaling.
- **Version Notes**: v1.29+ uses containerd, with unchanged HPA mechanics but `crictl` for debugging.

---

### 3. YAML Examples
Below are YAMLs for a Deployment, HPA, and load-testing pod.

#### Example 1: Deployment with Resource Requests
```yaml
# File: workloads/scalable-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
      - name: web
        image: nginx:1.25
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: "100m"
            memory: "128Mi"
          limits:
            cpu: "200m"
            memory: "256Mi"
```

**Critical Fields**:
- `resources.requests`: Enables CPU/memory metrics for HPA.

#### Example 2: Horizontal Pod Autoscaler
```yaml
# File: workloads/web-hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: web-hpa
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
    scaleUp:
      stabilizationWindowSeconds: 180
```

**Critical Fields**:
- `scaleTargetRef`: Targets `web-app` Deployment.
- `minReplicas`, `maxReplicas`: Scaling range.
- `metrics`: 80% CPU target.
- `behavior`: Custom stabilization windows.

#### Example 3: Load-Testing Pod
```yaml
# File: workloads/load-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: load-tester
  namespace: default
spec:
  containers:
  - name: tester
    image: busybox
    command: ["sh", "-c", "while true; do wget -q -O- http://web-app; sleep 1; done"]
```

**Purpose**: Simulate HTTP load on `web-app`.

#### Example 4: Debug Pod
```yaml
# File: workloads/debug-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: debug-pod
  namespace: default
spec:
  containers:
  - name: debug
    image: busybox
    command: ["sh", "-c", "sleep 3600"]
```

**Purpose**: Debug scaling issues.

---

### 4. Critical Commands
Key commands for autoscaling:

| Command | Description | Exam Tip |
|---------|-------------|----------|
| `kubectl autoscale deployment <name> --min=<n> --max=<n> --cpu-percent=<n>` | Create HPA. | Quick setup. |
| `kubectl apply -f <file>` | Apply HPA YAML. | Custom configs. |
| `kubectl get hpa` | List HPAs. | Check status. |
| `kubectl describe hpa <name>` | HPA events, metrics. | Debug scaling. |
| `kubectl get pods` | Pod count. | Verify scaling. |
| `kubectl describe deployment <name>` | Deployment state. | Check replicas. |
| `kubectl top pods` | Pod metrics. | Requires metrics-server. |
| `kubectl exec <pod> -- stress --cpu <n>` | Simulate CPU load. | Test HPA. |
| `kubectl logs <pod>` | App logs. | Debug app issues. |
| `crictl ps` | Pod states. | Debug crashes. |
| `crictl inspect <id>` | Resource limits. | Verify requests. |
| `journalctl -u kubelet` | Kubelet errors. | Metrics issues. |

---

### 5. Step-by-Step Procedures
Hereâ€™s how to configure, test, and troubleshoot autoscaling.

#### Scenario 1: Create and Test HPA
**Step 1: Deploy Application**
```bash
kubectl apply -f workloads/scalable-deployment.yaml
kubectl get pods
# Output: web-app-xyz   1/1   Running   (2 pods)
```

**Step 2: Create HPA**
```bash
kubectl apply -f workloads/web-hpa.yaml
kubectl get hpa
# Output: web-hpa   TARGETS   MINPODS   MAXPODS   REPLICAS
#                   <unknown>/80%   2         10        2
```

**Step 3: Simulate Load**
```bash
kubectl apply -f workloads/load-pod.yaml
kubectl exec load-tester -- sh -c "while true; do wget -q -O- http://web-app; sleep 0.1; done &"
```

**Step 4: Monitor Scaling**
```bash
kubectl get hpa --watch
# Output: web-hpa   85%/80%   2   10   3   (scales up)
kubectl get pods
# Output: web-app-xyz   1/1   Running   (3+ pods)
```

**Step 5: Verify**
```bash
kubectl describe hpa web-hpa
# Output: Events: Scaled up to 3 replicas
kubectl top pods
# Output: High CPU on web-app pods
```

#### Scenario 2: Debug HPA Not Scaling
**Step 1: Deploy Faulty Setup**
```bash
cat <<EOF > bad-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: bad-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: bad
  template:
    metadata:
      labels:
        app: bad
    spec:
      containers:
      - name: web
        image: nginx:1.25
        # Missing resources.requests
EOF
kubectl apply -f bad-deployment.yaml
kubectl autoscale deployment bad-app --min=2 --max=10 --cpu-percent=80
```

**Step 2: Check**
```bash
kubectl get hpa
# Output: bad-app   <unknown>/80%   2   10   2
kubectl describe hpa bad-app
# Events: "Failed to get CPU utilization: missing request for cpu"
```

**Step 3: Runtime Debugging**
```bash
kubectl apply -f workloads/debug-pod.yaml
kubectl exec -it debug-pod -- sh
crictl ps
# Output: bad-app pods Running
journalctl -u kubelet | grep metrics
# Output: No errors, but metrics-server needs requests
```

**Step 4: Fix**
```bash
kubectl edit deployment bad-app
# Add: resources.requests.cpu: 100m
kubectl get hpa --watch
# Output: bad-app   10%/80%   2   10   2   (metrics now available)
```

**Step 5: Test Scaling**
```bash
kubectl exec load-tester -- sh -c "while true; do wget -q -O- http://bad-app; sleep 0.1; done &"
kubectl get pods
# Output: bad-app-xyz   1/1   Running   (3+ pods after load)
```

#### Scenario 3: Validate Scaling
**Step 1: Deploy and Scale**
```bash
kubectl apply -f workloads/scalable-deployment.yaml
kubectl apply -f workloads/web-hpa.yaml
kubectl apply -f workloads/load-pod.yaml
kubectl exec load-tester -- sh -c "while true; do wget -q -O- http://web-app; sleep 0.1; done &"
```

**Step 2: Verify**
```bash
kubectl get hpa
# Output: web-hpa   90%/80%   2   10   4
kubectl get pods
# Output: web-app-xyz   1/1   Running   (4 pods)
kubectl describe hpa web-hpa
# Output: Events: Scaled up to 4 replicas
```

---

### 6. Exam-Style Questions
Below are CKA-style questions, with runtime debugging included.

#### Question 1: Task-Based (7 minutes)
**Task**: Create an HPA for a Deployment (2-10 replicas, 80% CPU).

**Steps**:
```bash
kubectl apply -f workloads/scalable-deployment.yaml
kubectl autoscale deployment web-app --min=2 --max=10 --cpu-percent=80
kubectl get hpa
# Output: web-hpa   <unknown>/80%   2   10   2
kubectl describe hpa web-hpa
# Output: Target: web-app, Metrics: cpu
```

#### Question 2: Troubleshooting (8 minutes)
**Task**: Fix an HPA not scaling due to missing metrics.

**Steps**:
```bash
kubectl apply -f bad-deployment.yaml
kubectl autoscale deployment bad-app --min=2 --max=10 --cpu-percent=80
kubectl get hpa
# Output: bad-app   <unknown>/80%   2   10   2

kubectl describe hpa bad-app
# Events: "missing request for cpu"

# Fix
kubectl edit deployment bad-app
# Add: resources.requests.cpu: 100m

kubectl get hpa
# Output: bad-app   10%/80%   2   10   2
```

#### Question 3: Validation (5 minutes)
**Task**: Verify HPA scaled pods after load.

**Steps**:
```bash
kubectl apply -f workloads/scalable-deployment.yaml
kubectl apply -f workloads/web-hpa.yaml
kubectl apply -f workloads/load-pod.yaml
kubectl exec load-tester -- sh -c "while true; do wget -q -O- http://web-app; sleep 0.1; done &"
kubectl get hpa
# Output: web-hpa   85%/80%   2   10   3
kubectl get pods
# Output: web-app-xyz   1/1   Running   (3 pods)
```

#### Question 4: Troubleshooting (7 minutes)
**Task**: Debug HPA not scaling due to missing metrics-server.

**Steps**:
```bash
kubectl apply -f workloads/scalable-deployment.yaml
kubectl apply -f workloads/web-hpa.yaml
kubectl get hpa
# Output: web-hpa   <unknown>/80%   2   10   2

kubectl describe hpa web-hpa
# Events: "Failed to get CPU metrics: metrics not available"

# Assume metrics-server installation (exam may provide)
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
kubectl get pods -n kube-system | grep metrics-server
# Output: metrics-server-xyz   1/1   Running

kubectl get hpa
# Output: web-hpa   10%/80%   2   10   2
```

---

### 7. Important Key Points to Remember
- **HPA**:
  - Scales pods based on CPU, memory, or custom metrics.
  - `minReplicas`, `maxReplicas`, `targetAverageUtilization`.
  - Requires `metrics-server`, `requests.cpu`.
- **Cluster Autoscaler**:
  - Scales nodes, supports HPA.
  - Uses taints/tolerations for scheduling.
- **Behavior**:
  - Stabilization windows: 3m (up), 5m (down).
  - Checks metrics every 15s.
- **Runtime Debugging**:
  - `crictl ps/inspect`: Pod issues.
  - `journalctl`: Metrics errors.
- **Exam Focus**:
  - Create HPAs, simulate load.
  - Fix metrics, requests issues.
  - Validate scaling.
- **Version Note**:
  - v1.29+: Containerd, same mechanics.

---

### 8. Common Mistakes to Avoid
- **Mistake**: Missing `requests.cpu` in Deployment.
  - **Fix**: Add `resources.requests.cpu`.
- **Mistake**: No metrics-server installed.
  - **Fix**: Deploy metrics-server (if allowed).
- **Mistake**: Wrong HPA target (e.g., wrong Deployment).
  - **Fix**: Verify `scaleTargetRef`.
- **Mistake**: Ignoring pod crashes during scale-up.
  - **Fix**: Check `kubectl logs`, `crictl`.
- **Mistake**: Setting `minReplicas` = `maxReplicas`.
  - **Fix**: Ensure range allows scaling.

**Exam Traps**:
- Missing `resources.requests`.
- Wrong namespace for HPA/Deployment.
- Not validating pod count after load.

---

### 9. Troubleshooting Tips
- **HPA Not Scaling**:
  - Check: `kubectl describe hpa`.
  - Causes: Missing metrics-server, no `requests.cpu`.
  - Fix: Deploy metrics-server, add requests.
- **Pods Not Scheduling**:
  - Check: `kubectl describe pod`.
  - Causes: Insufficient nodes, taints.
  - Fix: Check Cluster Autoscaler, tolerations.
- **Pod Crashes During Scale**:
  - Check: `kubectl logs`, `crictl logs`.
  - Causes: Resource limits, app errors.
  - Fix: Adjust limits, debug app.
- **Runtime Issues**:
  - Check: `crictl ps/inspect`.
  - Causes: Crashes, runtime errors.
  - Fix: Correct image, restart runtime.
- **Tools**:
  - `kubectl describe hpa`: Metrics issues.
  - `kubectl top`: Verify load.
  - `crictl`: Pod diagnostics.

**Debugging Checklist**:
1. Check HPA (`kubectl get hpa`).
2. Inspect events (`kubectl describe hpa`).
3. Verify Deployment (`kubectl describe deploy`).
4. Debug pods (`crictl inspect`).
5. Fix YAML (`kubectl edit`).
6. Validate (`kubectl get pods`).

---

### 10. Additional Resources
- **Kubernetes Docs**:
  - [Horizontal Pod Autoscaler](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)
  - [Metrics Server](https://github.com/kubernetes-sigs/metrics-server)
  - [Cluster Autoscaler](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler)
- **Practice Tools**:
  - **Minikube**: Test HPA with metrics-server.
  - **KillerCoda/KodeKloud**: CKA labs.
  - **Kind**: Multi-node scaling.
- **Community**:
  - CNCF Slack: #kubernetes-users, #cka-prep.
  - Kubernetes GitHub: HPA issues.
  - X posts: Search #KubernetesHPA, #CKA.

---

### 11. GitHub Repo Integration
Hereâ€™s how to organize this topic in your GitHub repo.

#### Folder Structure
```
cka-prep/
â”œâ”€â”€ storage/
â”œâ”€â”€ troubleshoot/
â”œâ”€â”€ workloads/
â”‚   â”œâ”€â”€ my-app-deployment.yaml
â”‚   â”œâ”€â”€ rolling-deployment.yaml
â”‚   â”œâ”€â”€ debug-pod.yaml
â”‚   â”œâ”€â”€ private-deployment.yaml
â”‚   â”œâ”€â”€ registry-secret.yaml
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ logging-daemonset.yaml
â”‚   â”œâ”€â”€ backup-cronjob.yaml
â”‚   â”œâ”€â”€ batch-job.yaml
â”‚   â”œâ”€â”€ sidecar-pod.yaml
â”‚   â”œâ”€â”€ init-pod.yaml
â”‚   â”œâ”€â”€ blue-deployment.yaml
â”‚   â”œâ”€â”€ green-deployment.yaml
â”‚   â”œâ”€â”€ blue-green-service.yaml
â”‚   â”œâ”€â”€ stable-deployment.yaml
â”‚   â”œâ”€â”€ canary-deployment.yaml
â”‚   â”œâ”€â”€ canary-service.yaml
â”‚   â”œâ”€â”€ rolling-update-deployment.yaml
â”‚   â”œâ”€â”€ recreate-deployment.yaml
â”‚   â”œâ”€â”€ app-configmap.yaml
â”‚   â”œâ”€â”€ app-secret.yaml
â”‚   â”œâ”€â”€ configured-pod.yaml
â”‚   â”œâ”€â”€ private-pod.yaml
â”‚   â”œâ”€â”€ immutable-configmap.yaml
â”‚   â”œâ”€â”€ config-pod.yaml
â”‚   â”œâ”€â”€ tls-secret.yaml
â”‚   â”œâ”€â”€ secret-pod.yaml
â”‚   â”œâ”€â”€ scalable-deployment.yaml
â”‚   â”œâ”€â”€ web-hpa.yaml
â”‚   â”œâ”€â”€ load-pod.yaml
â”‚   â”œâ”€â”€ README.md
â”œâ”€â”€ README.md
```

#### README Content (workloads/README.md, Updated)
```markdown
# Workloads & Scheduling: Autoscaling

## Theory
- **HPA**: Scales pods via CPU/memory metrics.
- **Cluster Autoscaler**: Scales nodes (context).

## Files
- `scalable-deployment.yaml`: Deployment with requests.
- `web-hpa.yaml`: HPA for scaling.
- `load-pod.yaml`: Simulates load.
- Others: `app-secret.yaml`, etc.

## Procedures
1. Deploy: `kubectl apply -f`
2. Create HPA: `kubectl autoscale`
3. Load: `kubectl exec load-tester`
4. Debug: `kubectl describe hpa`, `crictl`
5. Validate: `kubectl get pods`

## Key Points
- HPA needs `metrics-server`, `requests.cpu`.
- `minReplicas`, `maxReplicas`, 80% CPU.
- Stabilization: 3m up, 5m down.

## Common Mistakes
- No `requests.cpu`.
- Missing metrics-server.
- Wrong target.

## Troubleshooting
- No scaling? Check metrics, requests.
- Pods fail? Debug `crictl`, limits.

## Questions
1. Create HPA (2-10, 80% CPU).
2. Fix missing metrics.
3. Validate scaling.
4. Debug metrics-server.
```

#### File Comments (web-hpa.yaml)
```yaml
# web-hpa.yaml
# HPA for web-app Deployment
# Verify: kubectl get hpa --watch
# Use: Practice autoscaling, load testing
```

---

### Comprehensive Summary
This topic equips you to configure **workload autoscaling**, enabling dynamic scaling of applications with **Horizontal Pod Autoscaler (HPA)**. Youâ€™ve learned:
- How to create HPAs to scale pods based on CPU or memory metrics.
- How to simulate load and validate scaling behavior.
- How to troubleshoot HPA failures, including metrics and resource issues, using `kubectl` and `crictl`.
- Contextual knowledge of Cluster Autoscaler for node scaling.

**Practice Plan**:
- Deploy `scalable-deployment.yaml`, `web-hpa.yaml`, and `load-pod.yaml` (Minikube, Kind).
- Simulate failures: missing `requests.cpu`, no metrics-server. Debug with `kubectl describe`, `crictl`.
- Time yourself on the exam questions (<20 minutes total).
- Use KillerCoda/KodeKloud for HPA labs.

**Next Steps**:
- Since weâ€™ve covered all listed topics in Workloads & Scheduling, move to the next CKA section (e.g., **Cluster Architecture, Installation & Configuration**) or another topic if you have one in mind.
- Practice mixed scenarios (e.g., HPA + ConfigMaps).
- Let me know if you want more runtime debugging, HPA edge cases, or a full section review.

---

This response covers **Configure Workload Autoscaling** comprehensively, with runtime debugging included, tailored for your CKA prep and GitHub repo. Weâ€™ve nailed another topic in Workloads & Schedulingâ€”amazing work! Please share the next section or topic, and Iâ€™ll keep delivering. Any tweaks, more debugging, or specific autoscaling scenarios? Letâ€™s keep this prep phenomenal! ðŸ˜Š
